---
output: html_document 
---

# Reproduction of Malcomb et al (2014)

Malcomb, D. W., E. A. Weaver, and A. R. Krakowka. 2014.
Vulnerability modeling for sub-Saharan Africa: 
An operationalized approach in Malawi. *Applied Geography* 48:17-30.
DOI:[10.1016/j.apgeog.2014.01.004]([https://doi.org/10.1016/j.apgeog.2014.01.004)

### Reproduction Authors

Kufre Udoh, Drew An-Pham, Joseph Holler, and 
Middlebury College Geography 323 Classes (Fall 2019, Spring 2021, Fall 2021)
[https://gis4dev.github.io/](https://gis4dev.github.io/)


```{r install packages and load libraries, include = F}
packages = c("downloader", "haven", "stars", "dplyr", "sf", "rdhs", "classInt", 
             "readr", "ggplot2", "here", "s2", "pastecs", "cartography")

install.packages(setdiff(packages, rownames(installed.packages())),
                 quietly=TRUE)

library(downloader)
library(haven)
library(sf)
library(stars)
library(dplyr)
library(here)
library(classInt)
library(rdhs)
library(readr)
library(ggplot2)
library(s2)
library(pastecs)
library(cartography)

# Use spherical geometries for SF functions
sf_use_s2(TRUE)
```

```{r save R processing environment}
# save the R processing environment, including date in file name
env <- here("procedure", "environment")
if (!file.exists(here(env, "r_environment.txt"))) {
  writeLines(
    capture.output(sessionInfo()), here(env, "r_environment.txt")
  )
} else {
  writeLines(
    capture.output(sessionInfo()),
    here(env, paste0("r-environment-", Sys.Date(), ".txt"))
  )
}
```

```{r define raw data paths}
private_r <- here("data","raw","private")
public_r <- here("data","raw","public")
```

```{r load data for analysis, message=FALSE}
ta_gref <- read_sf(here("data", "derived", "public", "ta_fig4_v.gpkg"), layer = "ta_fig4_v") %>% 
  st_make_valid()
# This read may need to be changed to match the final output of 01-preanalysis!
# which was: st_write(ta_v, here("data", "derived", "public", "ta_v.gpkg"), append=FALSE)

dr <- read_stars(here(public_r, "dr1010ipeykx.tif")) %>% 
  st_set_crs(4326) 

fl <- read_stars(here(public_r,  "fl1010irmt.tif")) %>% 
  st_set_crs(4326) 

lhz_v <- read_sf(here(public_r, "MW_LHZ_2009.shp")) %>% 
  st_make_valid()

lhz_t <- read.csv(here(public_r, "lhz.csv"))

ta <- read_sf(here(public_r, "MWI_adm2.shp")) %>%
  st_make_valid() 

# it may be possible to simplify this -- just dissolving lakes into single
# multi-part feature
lakes <- read_csv(here(public_r, "major_lakes.csv")) %>% 
  select("name", "the_geom") %>% 
  st_as_sf(wkt = "the_geom", crs = 4326) %>%
  st_geometry %>%
  st_union %>%
  st_sf %>%
  mutate(EA = "Major Lakes of Malawi")
```

```{r load adaptive capacity data}
dhsclusters_2010 <- readRDS(here(private_r, "datasets", "MWGE62FL.rds")) %>%
  as("sf") # convert to simple features

dhshh_2010 = readRDS(here(private_r, "datasets", "MWHR61SV.rds")) %>%
  zap_labels()
```

```{r join traditional authority and cluster data to households}
# spatial join traditional authority ID to clusters
cluster_ta <- dhsclusters_2010 %>% 
  st_join(ta_gref) %>% 
  st_drop_geometry() %>% 
  select(DHSCLUST, ta_id = ID_2, urban_rural = URBAN_RURA)

# join traditional authority and urban/rural data to DHS surveys
# select variables for adaptive capacity analysis
dhshh_2010 <- dhshh_2010 %>%
  left_join(cluster_ta, by = c("HV001" = "DHSCLUST")) %>% 
  select(
    HHID,
    HV001,
    HV002,
    ta_id,
    urban_rural,
    HV246A,
    HV246D,
    HV246E,
    HV246G,
    HV248,
    HV245,
    HV271,
    HV251,
    HV204,
    HV206,
    HV226,
    HV219,
    HV243A,
    HV207
  )

paste(sum(is.na(cluster_ta$ta_id)),
      "clusters did not join to a Traditional Authority, affecting",
      sum(is.na(dhshh_2010$ta_id)),
      "household surveys")
```

```{r households to remove}
# create list of households with inconclusive data for adaptive capacity
# that is, the household gave a "do not know" answer to a question
rmv_2010 <- dhshh_2010 %>%  
  filter(
    HV246A == 98 |
    HV246A == 99 |
    HV246D == 98 |
    HV246D == 99 |
    HV246E == 98 |
    HV246E == 99 |
    HV246G == 98 |
    HV246G == 99 |
    HV219  == 9 |
    HV243A == 9 |
    HV245  == 99 |
    HV206  == 9 |
    HV204  == 999 |
    HV204  == 998 |
    HV226  == 99 |
    HV226  == 95 |
    HV226  == 96 |
    HV207  == 9 |
    (is.na(HV246A) & is.na(HV246D) & is.na(HV246E) & is.na(HV246G))
  ) 
paste(nrow(rmv_2010), "household surveys with inconclusive attribute data")
```

```{r remove missing data}
paste(nrow(dhshh_2010), "household surveys")
# count missing ta_id's

hh_capacity_t <- dhshh_2010 %>%
  anti_join(rmv_2010, by = "HHID") %>% # 24638 records
  filter(!is.na(ta_id)) # 23543 records

paste(nrow(hh_capacity_t), "household surveys with complete data")
paste(nrow(dhshh_2010)-nrow(hh_capacity_t), "surveys removed for missing data")
```
    
```{r capacity in households 2010}
hh_capacity_t <- hh_capacity_t  %>%
  rowwise() %>%
  mutate(hhlivestock = sum(HV246A, HV246D, HV246E, HV246G, na.rm = T)) %>%
  ungroup() %>%
  mutate(
    livestock = percent_rank(hhlivestock) * 4,
    sick = percent_rank(desc(HV248)) * 4,
    land = percent_rank(HV245) * 4,
    wealth = percent_rank(HV271) * 4,
    orphans = percent_rank(desc(HV251)) * 4,
    HV204 = ifelse(HV204 == 996, 0, HV204), # change 996 (water on premises) to 0
    water = percent_rank(desc(HV204)) * 4,
    electricity = percent_rank(HV206) * 4,
    cooking = percent_rank(desc(HV226)) * 4,
    femalehh = percent_rank(desc(HV219)) * 4, # male = 1 and female = 2
    cellphone = percent_rank(desc(HV243A)) * 4,
    radio = percent_rank(HV207) * 4,
    urban_rural = ifelse(urban_rural == "U", 1, 0),
    urbanruralscore = percent_rank(urban_rural) * 4
  ) 
  # percent_rank sets values 0 to 1, then * 4 puts everything on a 0-4 scale
  # default is ascending order, with lower values being more vulnerable
  # desc() option inverts order, with higher values being more vulnerable
```

Calculate household-level adaptive capacity scores based on Table 2 weights.

```{r capacity for traditional authorities 2010 at the household level}
hh_capacity_t <- mutate(hh_capacity_t, 
                        capacity = 
                        0.04 * livestock +
                        0.03 * sick +
                        0.06 * land +
                        0.04 * wealth +
                        0.03 * orphans +
                        0.04 * water +
                        0.03 * electricity +
                        0.02 * cooking +
                        0.02 * femalehh +
                        0.04 * cellphone +
                        0.03 * radio +
                        0.02 * urbanruralscore
                        )
```

``` {r summary stats for tas at hh level before pullings nas}
hh_stats <- stat.desc(hh_capacity_t[,21:33]) %>%
  mutate_if(is.numeric, round, digits=1)
hh_stats
```

```{r households with incomplete capacity data}
hh_null <- filter(hh_capacity_t, is.na(capacity))
paste(nrow(hh_null), "household surveys with null data")
hh_null_stats <- stat.desc(hh_null[,21:32]) %>%
  mutate_if(is.numeric, round, digits=1)
hh_null_stats
```

```{r remove incomplete households}
hh_capacity_t <- hh_capacity_t %>%
filter(!is.na(capacity)) 
paste(nrow(hh_capacity_t), "household surveys with complete adaptive capacity")
```

``` {r summary statistics for household adaptive capacity}
hh_capacity_stats <- stat.desc(hh_capacity_t[,21:33]) %>%
  mutate_if(is.numeric, round, digits=1)
hh_capacity_stats
```

```{r aggregate adaptive capacity to traditional authorities}
ta_capacity_t <- hh_capacity_t %>% 
  group_by(ta_id) %>%
  summarize(
    capacity_avg = mean(capacity),
    capacity_min = min(capacity),
    capacity_max = max(capacity),
    capacity_sd = sd(capacity)
  )
ta_capacity_t
paste(nrow(ta_capacity_t),
      "traditional authorities with adaptive capacity data")
```

```{r joining 2010 capacity to ta and creating breaks for visualization}
# join mean capacity to traditional authorities
ta_v = left_join(
  ta_gref,
  select(ta_capacity_t, ta_id, capacity_2010 = capacity_avg),
  by = c("ID_2" = "ta_id")
)

# indicators ranged 0-4 and were multiplied by weights totaling 0.4
# yielding maximum theoretical score of 1.6; therefore multiply by 25
# to achieve a maximum score of 40
ta_v = mutate(ta_v, capacity_2010 = capacity_2010 * 25)

# preparing breaks for mapping using natural jenks method
ta_brks = filter(ta_v, !is.na(capacity_2010)) %>% {classIntervals(.$capacity_2010, 4, style = "jenks")$brks}

ta_int = lapply(1:4, function(x) paste0(round(ta_brks[x],2)," - ", round(ta_brks[x +1],2))) %>% unlist()

ta_v = mutate(ta_v, capacity_2010_brks = case_when(
  capacity_2010 <= ta_brks[2] ~ ta_int[1],
  capacity_2010 <= ta_brks[3] ~ ta_int[2],
  capacity_2010 <= ta_brks[4] ~ ta_int[3],
  capacity_2010 >  ta_brks[4] ~ ta_int[4]
))
```

```{r extract traditional authorities without data by capacity, warning=FALSE}
ta_null_cap <- ta_v[is.na(ta_v$capacity_2010),]
#20 features

st_write(
  ta_null_cap,
  here("data", "derived", "public", "ta_null_capacity.shp"), 
  append = FALSE,
  quiet = TRUE
)
```

```{r saving adaptive capacity scores}
save(
  ta_capacity_t,
  file = here("data", "derived", "public", "adaptive_capacity.rData")
)
```

```{r check final ta_v output before mapping}
# this should be classified with natural breaks, similar to the publication
# adjust x axis labelling
ta_v_check <- ggplot() + 
  geom_sf(data = ta_v, aes(fill = capacity_2010), size = .1, color = "grey98") + 
  scale_fill_continuous(
    name = "Adaptive\nCapacity\nScores", 
    low="lemonchiffon1", 
    high="lightcoral", 
    na.value = "snow3") 
ta_v_check 
```

```{r cleaning and reprojecting rasters}
# creating blank raster in extent
# extent of the raster layers
# units used for bounding box = decimal degrees, long-lat
b = st_bbox(
  c(
    xmin = 35.9166666666658188,
    xmax = 32.6666666666658330,
    ymin = -9.3333333333336554,
    ymax = -17.0833333333336270
  ),
  crs = st_crs(4326)
) %>%
  st_as_sfc()

# where raster = 1, you have flood visualized
# alternative resolution: 0.083333
blank = st_as_stars(st_bbox(b), dx = 0.041667, dy = 0.041667)
blank[[1]][] = NA

# reprojecting, clipping, and resampling rasters to new extent and cell size
# use bilinear for drought to average continuous population exposure values
# (object, destination, logical operator, usable b/c of GDAL)
dr = st_warp(dr, blank, use_gdal = T, method = "bilinear")
# use nearest neighbor for flood risk to preserve integer values
fl = st_warp(fl, blank, method = "near")  

# removing factors from fl
# You can use double brackets to select elements in more or less the same way as single brackets. The difference between single and double is that with double brackets any element names are not displayed.
# rasters are a raised matrix (multiple layer) access first level of the array, second bracket editing values into the array
nmrc = as.numeric(levels(fl[[1]]))[fl[[1]]]
fl = blank
# single bracket gives you meta data (first layer), double brackets gives it to you as a matrix
fl[[1]][] = nmrc
```

```{r rasterizing geometries}
# clipping traditional authorities with livelihood zones in order to remove lake and existing null values
# building a clip function in R [x,y are parameters]
st_clip = function(x,y) st_intersection(x, st_union(st_geometry(y)))

st_agr(ta_v) = "constant"

#creates a small buffer around the Livelihood Zones & use this as a mask to clip features from Traditional Authorities (only the TAs that have data clipped to the extent of Malawi)
ta_capacity_v = st_clip(st_transform(filter(ta_v, is.na(capacity_2010) == F), 3395), st_buffer(st_transform(lhz_v, 3395), .01)) %>%
  st_transform(4326)
# 214 features 

# making capacity rasters 
ta_capacity_r = st_rasterize(ta_capacity_v[, 'capacity_2010'], blank)
```

Calculate livelihood sensitivity data with FEWSnet livelihood zones according 
to table 2.

```{R calculate livelihood sensitivity}
# LHZ Data
lhz_sensitivity_v  = lhz_v %>%
  inner_join(lhz_t, by = ("LZCODE" = "LZCODE")) %>%
  mutate(
    pctOwnCrop = (sof_crop + sof_livestock) * 100, 
    pctIncWage = (soc_labour / soc_total) * 100,
    pctIncCashCrops = (cp_tobacco + cp_sugar + cp_tea + cp_coffee) / soc_total 
      * 100,
    pctDisasterCope = (se_firewoord + se_grass	+ se_wildfood + se_charcoal + 
      se_matmaking + se_basket) / soc_total * 100
  ) %>%
  mutate(
    ownCrop = percent_rank(pctOwnCrop) * 4, 
    # high ownCrop percentages indicate low sensitivity  
    wageIncome = percent_rank(pctIncWage) * 4, 
    # high wage percentages indicate low sensitivity
    cashCropIncome = percent_rank(desc(pctIncCashCrops)) * 4, 
    # high cash crop percentages indicate high sensitivity (market exposure)
    disasterCope = percent_rank(desc(pctDisasterCope)) * 4, 
    # high disaster coping strategy percentages indicate high sensitivity 
  ) %>%
  mutate(
    sensitivity = (
      0.06 * ownCrop +
      0.06 * wageIncome +
      0.04 * cashCropIncome +
      0.04 * disasterCope
      ) * 25
  ) 
# livelihood sensitivity range is 5.88 to 14.00
# making capacity rasters 

# can we use select() rather than the brackets?
lhz_sensitivity_r = st_rasterize(lhz_sensitivity_v[,'sensitivity'], blank)

lhz_v_check <- ggplot() + 
  geom_sf(data = lhz_sensitivity_v, 
          aes(fill = sensitivity), size = .2, color = "grey98") +
  scale_fill_continuous(name = "Livelihood\nSensitivity\nScores", low="lemonchiffon1", high="lightcoral")
lhz_v_check
```

```{r summary table with lhz data}
lhz_stats <- st_drop_geometry(lhz_sensitivity_v[,14:36]) %>%
  stat.desc() %>%
  mutate_if(is.numeric, round, digits=1)
lhz_stats
```

```{r function to calculate vulnerability}
  # creating mask layer
  mask = ta_capacity_r
  mask[mask > 0] = 1
  mask[mask == 0] = NA
  
  # masking flood and drought 
  #reclassify drought & flood already has classification
  #we want flood to have 20% weight so we multiply by 5, as is the data has a range of 0-4
  flood = fl * mask * 5
  drought = dr * mask
  
  # makes a list of quintile break points
  qt = quantile(drought[[1]], probs = seq(0, 1, 0.2), na.rm = T)
  
  # reclassifying drought layer using break points from 0 to 4
  # 4 * 5 = 20, match 20% weighting of exposition to drought events
  drought = drought %>%
    mutate(
      recoded = case_when(
        drought[[1]] <= qt[[2]] ~ 0,
        drought[[1]] <= qt[[3]] ~ 1,
        drought[[1]] <= qt[[4]] ~ 2,
        drought[[1]] <= qt[[5]] ~ 3,
        drought[[1]] > qt[[5]] ~ 4
      )
    ) %>% select(recoded) * 5
  
  # final output (adding component rasters)
  final = 100 - (ta_capacity_r + lhz_sensitivity_r + (40 - (drought + flood))) # 100 = highest vulnerability, 0 = lowest vulnerability
```

```{r descriptive stats for raster inputs}
# these stats are not formatted usefully 
fl_stats <- stat.desc(fl)
dr_stats <- stat.desc(dr)
fl_stats
dr_stats
```

```{r creating final vulnerability layers}
ta_capacity_v <- st_make_valid(ta_capacity_v) # new version of sf (1.0.0) produces error, so st_make_valid fixes spherical geom errors

ta_capacity_v$vuln = aggregate(final,ta_capacity_v,mean)$capacity_2010 

# rasterize 
ta_r = ta_capacity_v[!is.na(ta_capacity_v$vuln), "vuln"]
ta_r = st_rasterize(ta_r[1])
```

```{r extract traditional authorities without data by vuln, warning=FALSE}
ta_null <- ta_capacity_v[is.na(ta_capacity_v$vuln),]
#20 features

st_write(
  ta_null,
  here("data", "derived", "public", "ta_null.shp"),
  append = FALSE,
  quiet = TRUE)
```

```{r misc. map features}
#ea used to retrieve national parks (conservation areas) + lakes
ea = lhz_v %>%
  st_transform(3395) %>%  #transform to world mercator (jh: not sure if we need to transform to 3395 and back here?)
  summarize %>%  
  st_geometry %>%  #dissolve to one feature / one geometry
  st_intersection(st_geometry(st_transform(ta, 3395))) %>%   #intersect with traditional authorities to clip them
  st_transform(4326) %>%
  st_sf %>%   #make into new simple features data frame
  mutate(EA = case_when(
    grepl("Reserve", ta[["NAME_2"]]) | grepl("Park", ta[["NAME_2"]]) ~ "National Parks and Reserves",
    T ~ "Missing Data") # search and replace names anything with Reserve or Park in the name becomes National Parks and Reserves
  ) %>%
  rbind(lakes) %>%
  st_make_valid()
```

```{r 2010 adaptive capacity map}
# create map legend, starting with natural breaks intervals created earlier
map_2010legend <- c("#333333","#666666","#999999","#CCCCCC")  
names(map_2010legend) <- ta_int 
map_2010legend <- append(map_2010legend, c(
    "Missing Data" = "#FFC389",
    "National Parks and Reserves" = "#D9EABB",
    "Major Lakes of Malawi" = "lightblue"
  ))

# create the map
map_2010 = ggplot() +
  geom_sf(data = ea,
          aes(fill = EA),
          color = NA) +
  geom_sf(
    data = ta_capacity_v,
    aes(fill = capacity_2010_brks),
    color = "white",
    lwd = .2
  ) + scale_fill_manual(values = map_2010legend) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Adaptive Capacity Scores of\n222 Traditional Authorities in 2010") +
  theme_minimal() +
  theme(legend.title = element_blank())

map_2010
```

```{r find break & limit values for vulnerability map}
vuln_max <- max(final[[1]],na.rm = TRUE)
vuln_min <- min(final[[1]],na.rm = TRUE)
``` 

```{r vulnerability map}
clrs = mutate(
  ea,
  colors = case_when(
    EA == "Missing Data" ~ "#999999",
    EA == "National Parks and Reserves" ~ "#D9EABB",
    EA == "Major Lakes of Malawi" ~ "lightblue"
  )
)$colors

vuln_map = ggplot() +
  geom_sf(data = ea,
          fill = clrs,
          color = NA) +
  geom_stars(data = final) +
  scale_fill_gradient(
    low = "#FFFF75",
    high = "#CF4611",
    breaks = c(vuln_min, vuln_max),
    labels = c("Lower Vulnerability", "Higher Vulnerability"),
    na.value = "transparent",
    guide = "colourbar",
    limits = c(vuln_min, vuln_max)
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Malawi Vulnerability to Climate Change") +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

vuln_map
```

```{r saving fig 4 and 5 maps}

ggsave(
  here("results","figures","fig4rep_final.png"),
  plot = map_2010,
  width = 8.5,
  height = 11,
  units = "in"
)
ggsave(
  here("results","figures","fig5rep_final.png"),
  plot = vuln_map,
  width = 8.5,
  height = 11,
  units = "in"
)
```

```{r saving spatial data outputs}
results = here("data","derived","public","results.gpkg")

write_stars(ta_r, here("data","derived","public","ta_capacity.tif"))

write_sf(ta_capacity_v, results, "ta_capacity_v")

write_sf(lhz_v, results, "lhz")
```

```{R Sample Code for Comparing Discrete Choropleth Maps, warning=FALSE}
or_fig4 = # load original figure 4 data
  read_sf(here("data", "derived", "public", "ta_fig4_v.gpkg"), 
          layer="ta_fig4_v") %>% 
  # load ta_resilience layer from georeferencing geopackage
  st_drop_geometry() %>%
  # remove the geometry data because two geometries cannot be joined
  select(c(ID_2,capacity)) %>%  
  # select only the ID_2 and resilience columns
  na.omit()
  # remove records with null values

rp_fig4 = ta_capacity_v %>% # prepare our reproduction of figure 4 data
  select(c(ID_2,capacity_2010)) %>%  
  # select only the ID_2 and resilience columns
  # note: geometry columns are 'sticky' -- only way to remove is st_drop_geometry()
  na.omit() %>%
  # remove records with null values
  mutate(rp_ac = case_when(
    capacity_2010 <= ta_brks[2] ~ 1,
    capacity_2010 <= ta_brks[3] ~ 2,
    capacity_2010 <= ta_brks[4] ~ 3,
    capacity_2010 >  ta_brks[4] ~ 4
    )
  )
# code the capacity scores as integers, as we see them classified on the map. 
#ta_brks was the result of a Jenks classification, as noted on Malcomb et al's maps

fig4compare = inner_join(rp_fig4,or_fig4,by="ID_2") %>%  
  #inner join on field ID_2 keeps only matching records
  filter(rp_ac>0 & rp_ac<5 & capacity > 0 & capacity < 5)
  # keep only records with valid capacity scores

table(fig4compare$capacity,fig4compare$rp_ac)
# crosstabulation with frequencies

cor.test(fig4compare$capacity,fig4compare$rp_ac,method="spearman")
# Spearman's Rho correlation test

fig4compare = mutate(fig4compare, difference = rp_ac - capacity) 
# Calculate difference between the maps so that you can create a difference map
```
In order to compare the raster map produced with figure 5 from Malcomb et al (2014), figure 5 was georeferenced creating a ta_capacity.tif, which was then converted into polygons with the raster pixels to polygons tool. The authors used a color ramp of yellow to red in their raster map, so the best way to extract a linear relationship for comparison as we found was to add the blues and greens together in QGIS via zonal stats. After, the georeferenced TA's with color information were exported as a geopackge: georef_bg.gpkg and read into R for analysis.

```{R Sample Code for Comparing Continuous Raster Maps, warning=FALSE}
orfig5vect = 
  read_sf(here("data", "derived", "public", "georef_bg.gpkg"), 
          layer="georef_bg") %>%
  mutate(bg_mean = bmean + gmean)
# load original georeferenced figure 5 data

orfig5rast = st_rasterize(orfig5vect["bg_mean"], template=ta_r)
# convert mean of blue and green values into a raster using ta_final as a reference for raster
# extent, cell size, CRS, etc.

orfig5rast = orfig5rast %>% 
  mutate(or = 1-
           (bg_mean - min(orfig5rast[[1]], na.rm= TRUE)) /
           (max(orfig5rast[[1]], na.rm= TRUE) -
            min(orfig5rast[[1]], na.rm= TRUE)
        )
)  # or is Re-scaled from 0 to 1 with (value - min)/(max - min)
# it is also inverted, because higher blue values are less red

final = final %>% 
  mutate(rp =
           (capacity_2010 - min(final[[1]], na.rm= TRUE)) /
           (max(final[[1]], na.rm= TRUE) -
            min(final[[1]], na.rm= TRUE)
        )
)  # rp is Re-scaled from 0 to 1 with (value - min)/(max - min)

# FROM HERE, CODE COMPARING REPRODUCTION TO ORIGINAL DOES NOT WORK
# ROOT OF PROBLEM APPEARS TO BE RASTER RESOLUTION / RESAMPLING
fig5comp = c( select(final,"rp"), select(orfig5rast,"or"))
# combine the original (or) fig 5 and reproduced (rp) fig 5

fig5comp = fig5comp %>% mutate( diff = rp - or )
# calculate difference between the original and reproduction,
# for purposes of mapping

fig5comppts = st_as_sf(fig5comp)
# convert raster to vector points to simplify plotting and correlation testing

plot(fig5comppts$or, fig5comppts$rp, xlab="Original Study", ylab="Reproduction")
title("Comparing Vulnerability Scores")
# create scatterplot of original results and reproduction results

cor.test(fig5comppts$or, fig5comppts$rp, method="spearman")
# Spearman's Rho correlation test

# Hint for mapping raster results: refer to the diff raster attribute
# in the fig5comp stars object like this: fig5comp["diff"]
```

```{r 2010 adaptive capacity difference map}
ac_diff_map = ggplot() +
  geom_sf(data = ea,
          aes(fill = EA),
          color = NA) +
  geom_sf(
    data = fig4compare,
    aes(fill = factor(difference)),
    color = "white",
    lwd = .2
  ) + scale_fill_manual(limits = c("-2","-1","0","1","Missing Data","Major Lakes of Malawi","National Parks and Reserves"),
    values = list(
      "Missing Data" = "#FFFFFF",
      "National Parks and Reserves" = "#D9EABB",
      "Major Lakes of Malawi" = "lightblue",
      "-2" = "#e66101",
      "-1" = "#fdb863",
      "0" = "#cccccc",
      "1" = "#b2abd2"
    )
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Fig. 4 Replication Comparison") +
  theme_minimal() +
  theme(legend.title = element_blank())

ac_diff_map
```

```{r find break & limit values for vulnerability difference map}
abs(max(fig5comp[[3]],na.rm = TRUE))
abs(min(fig5comp[[3]],na.rm = TRUE))
range <- max(abs(max(fig5comp[[3]],na.rm = TRUE)),abs(min(fig5comp[[3]],na.rm = TRUE)))
# takes the greatest value and makes it the range
``` 

```{r vulnerability difference map}
clrs = mutate(
  ea,
  colors = case_when(
    EA == "Missing Data" ~ "#999999",
    EA == "National Parks and Reserves" ~ "#D9EABB",
    EA == "Major Lakes of Malawi" ~ "lightblue"
  
  )
)$colors
vuln_diff_map = ggplot() +
  geom_sf(data = ea,
          fill = clrs,
          color = NA) +
  geom_stars(data = fig5comp["diff"]) +
  scale_fill_gradient2(
    low = "#e9a3c9",
    high = "#beaed4",
    breaks = c(-range,range),
    labels = c("Negative Difference", "Positive Difference"),
    na.value = "transparent",
    guide = "colourbar",
    limits = c(-range,range) #you want centered around zero, so make larger # the ends
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Fig. 5 Replication Comparison") +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )
vuln_diff_map
```

```{r saving maps}
ggsave(
  here("results","figures","fig4comp_final.png"),
  plot = ac_diff_map,
  width = 8.5,
  height = 11,
  units = "in"
)
ggsave(
  here("results","figures","fig5comp_final.png"),
  plot = vuln_diff_map,
  width = 8.5,
  height = 11,
  units = "in"
)
```

```{r exporting comparison as geotiff}
write_stars(fig5comp, here("data", "derived", "public", "fig5diff.tif"), layer = "diff")
write_stars(fig5comp, here("data", "derived", "public", "fig5or.tif"), layer = "or")
write_stars(fig5comp, here("data", "derived", "public", "fig5rp.tif"), layer = "rp")
```